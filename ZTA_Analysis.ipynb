{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ZTA Survey Analysis\n",
        "\n",
        "本 Notebook 覆盖如下步骤：\n",
        "\n",
        "1. 数据清洗与转换（缺失值、Yes/No、Likert 重编码）\n",
        "2. 信度分析（Cronbach’s Alpha）\n",
        "3. 探索性因子分析（EFA）\n",
        "4. 描述统计（人口统计、模型变量）\n",
        "5. 相关分析（ZTA 熟悉度、必要性、满意度等）\n",
        "6. ANOVA/卡方（组织规模、行业与采用差异）\n",
        "\n",
        "结果文件输出至 `analysis_outputs/`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 基础依赖导入与配置\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tabulate import tabulate\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from factor_analyzer.factor_analyzer import FactorAnalyzer\n",
        "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
        "import pingouin as pg\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.width', 160)\n",
        "\n",
        "OUTPUT_DIR = 'analysis_outputs'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "DATA_PATH = 'Cleaned Data-5.20.xlsx'\n",
        "assert os.path.exists(DATA_PATH), f\"未找到数据文件: {DATA_PATH}\"\n",
        "\n",
        "print('环境与依赖加载完成。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 读取数据\n",
        "raw = pd.read_excel(DATA_PATH)\n",
        "df = raw.copy()\n",
        "print(df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qualtrics 预清洗：删除问卷说明/标签行，并规范完成样本\n",
        "if 'Progress' in df.columns:\n",
        "    # 仅保留 Progress 可数值化的行（去除如“进度”等标签行）\n",
        "    _prog_num = pd.to_numeric(df['Progress'], errors='coerce')\n",
        "    df = df[_prog_num.notna()].copy()\n",
        "\n",
        "# 去除显然的“说明文本行”\n",
        "text_like_cols = [c for c in df.columns if c.endswith('_TEXT')]\n",
        "for c in text_like_cols:\n",
        "    # 如果该列存在与问题描述高度相似的长文本且仅出现一次，通常为说明行，直接忽略该列在筛选中的影响\n",
        "    pass\n",
        "\n",
        "# 统一索引\n",
        "df = df.reset_index(drop=True)\n",
        "print('预清洗完成，样本量: ', len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数值化：基于比率判定列为 Likert(1-7) 或 Yes/No，并分别转换（修复索引对齐问题）\n",
        "for col in df.columns:\n",
        "    s = df[col]\n",
        "    if s.dtype == 'object':\n",
        "        mask = s.notna()\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        vals = s[mask].astype(str).str.strip()\n",
        "        low = vals.str.lower()\n",
        "        # 比例：纯数字1-7\n",
        "        numeric_ratio = (low.str.fullmatch(r'[1-7]').sum()) / len(vals)\n",
        "        # 比例：Yes/No\n",
        "        yesno_ratio = (low.isin(['yes','no','y','n','是','否']).sum()) / len(vals)\n",
        "        s_new = s.copy()\n",
        "        if numeric_ratio >= 0.5:\n",
        "            s_new.loc[mask] = pd.to_numeric(vals, errors='coerce')\n",
        "        elif yesno_ratio >= 0.5:\n",
        "            ymap = {'yes':1, 'no':0, 'y':1, 'n':0, '是':1, '否':0}\n",
        "            s_new.loc[mask] = low.map(ymap)\n",
        "        df[col] = s_new\n",
        "\n",
        "print('数值化完成。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 自动构念检测与关键变量映射（可按需手动修改）\n",
        "AUTO_CONSTRUCTS = {\n",
        "    'familiarity': [c for c in df.columns if str(c).startswith('Q3.1_')],\n",
        "    'necessity': [c for c in df.columns if str(c).startswith('Q3.2_')],\n",
        "    'satisfaction_current': [c for c in df.columns if str(c).startswith('Q2.9_')],\n",
        "    'satisfaction_previous': [c for c in df.columns if str(c).startswith('Q2.12_')],\n",
        "    'perceived_benefits': [c for c in df.columns if str(c).startswith('Q5.1_')],\n",
        "}\n",
        "\n",
        "# 手动可覆盖的构念（如有多题项量表，将题项列名放这里）\n",
        "CONSTRUCTS = {\n",
        "    # 'pts': ['PTS1','PTS2','PTS3'],\n",
        "    # 'ce': ['CE1','CE2','CE3'],\n",
        "}\n",
        "\n",
        "# 将自动构念映射到通用键以便后续分析（命名尽量兼容）\n",
        "CONSTRUCTS['pb'] = AUTO_CONSTRUCTS['perceived_benefits']\n",
        "# 若有 Q4.* 作为采纳意向条目，可启用：\n",
        "CONSTRUCTS['ai'] = [c for c in df.columns if str(c).startswith('Q4.')]\n",
        "\n",
        "KEY_VARS = {\n",
        "    'zta_familiarity': 'Q3.1_1',\n",
        "    'zta_necessity': 'Q3.2_1',\n",
        "    'zta_satisfaction': 'Q2.9_1',\n",
        "    'zta_adoption': 'Q2.8',\n",
        "    'org_size': 'Q1.2',\n",
        "    'industry': 'Q1.4'\n",
        "}\n",
        "\n",
        "print('构念/关键变量自动配置完成：')\n",
        "print({k: v for k, v in KEY_VARS.items() if v in df.columns})\n",
        "print('CONSTRUCTS 预览：', {k: len(v) for k, v in CONSTRUCTS.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据清洗 & 转换辅助函数\n",
        "LIKERT_MAP = {\n",
        "    'Strongly disagree': 1, 'Disagree': 2, 'Somewhat disagree': 3,\n",
        "    'Neutral': 4,\n",
        "    'Somewhat agree': 5, 'Agree': 6, 'Strongly agree': 7,\n",
        "    # 中文或数字化版本可扩展\n",
        "    '非常不同意': 1, '不同意': 2, '有点不同意': 3,\n",
        "    '中立': 4,\n",
        "    '有点同意': 5, '同意': 6, '非常同意': 7\n",
        "}\n",
        "\n",
        "YESNO_MAP = {\n",
        "    'Yes': 1, 'No': 0,\n",
        "    '是': 1, '否': 0,\n",
        "    'Y': 1, 'N': 0\n",
        "}\n",
        "\n",
        "NA_TOKENS = {'NA', 'N/A', 'na', 'n/a', 'none', 'None', 'NULL', 'null', ''}\n",
        "\n",
        "\n",
        "def normalize_missing(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    if isinstance(x, str) and x.strip() in NA_TOKENS:\n",
        "        return np.nan\n",
        "    return x\n",
        "\n",
        "\n",
        "def recode_likert(series: pd.Series) -> pd.Series:\n",
        "    return series.map(LIKERT_MAP).astype('float') if series.dtype == 'object' else series\n",
        "\n",
        "\n",
        "def recode_yesno(series: pd.Series) -> pd.Series:\n",
        "    return series.map(YESNO_MAP).astype('float') if series.dtype == 'object' else series\n",
        "\n",
        "\n",
        "def cronbach_alpha(df_items: pd.DataFrame) -> float:\n",
        "    # df_items: 每列为一个题项，数值型\n",
        "    df_items = df_items.dropna(axis=0, how='any')\n",
        "    k = df_items.shape[1]\n",
        "    if k < 2:\n",
        "        return np.nan\n",
        "    item_vars = df_items.var(axis=0, ddof=1)\n",
        "    total_var = df_items.sum(axis=1).var(ddof=1)\n",
        "    if total_var == 0:\n",
        "        return np.nan\n",
        "    alpha = (k / (k - 1)) * (1 - item_vars.sum() / total_var)\n",
        "    return float(alpha)\n",
        "\n",
        "print('清洗工具函数已定义。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 自动数据清洗：标准化缺失、修剪空白、统一大小写\n",
        "for col in df.columns:\n",
        "    df[col] = df[col].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "    df[col] = df[col].apply(lambda x: x.title() if isinstance(x, str) else x)  # 统一首字母大写风格\n",
        "    df[col] = df[col].apply(normalize_missing)\n",
        "\n",
        "print('缺失标准化完成。')\n",
        "\n",
        "# 尝试自动识别 Likert/YesNo 列进行重编码（保留原列做 _raw 备份）\n",
        "likert_like_tokens = set([k.title() if isinstance(k, str) else k for k in LIKERT_MAP.keys()])\n",
        "yesno_tokens = set([k.title() if isinstance(k, str) else k for k in YESNO_MAP.keys()])\n",
        "\n",
        "recode_report = []\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        values = set(v for v in df[col].dropna().unique())\n",
        "        # 判定为 Likert\n",
        "        if len(values & likert_like_tokens) >= 3:\n",
        "            df[f'{col}_raw'] = df[col]\n",
        "            df[col] = recode_likert(df[col])\n",
        "            recode_report.append((col, 'Likert'))\n",
        "        # 判定为 Yes/No\n",
        "        elif len(values & yesno_tokens) >= 2:\n",
        "            df[f'{col}_raw'] = df[col]\n",
        "            df[col] = recode_yesno(df[col])\n",
        "            recode_report.append((col, 'Yes/No'))\n",
        "\n",
        "pd.DataFrame(recode_report, columns=['column', 'type']).to_csv(os.path.join(OUTPUT_DIR, 'recode_report.csv'), index=False)\n",
        "print(f'自动重编码完成，共 {len(recode_report)} 列。')\n",
        "\n",
        "# 保存清洗后的数据（优先 Parquet，缺少引擎时回退 CSV）\n",
        "clean_parquet = os.path.join(OUTPUT_DIR, 'cleaned_dataset.parquet')\n",
        "clean_csv = os.path.join(OUTPUT_DIR, 'cleaned_dataset.csv')\n",
        "try:\n",
        "    df.to_parquet(clean_parquet, index=False)\n",
        "    print('清洗数据已保存至: ', clean_parquet)\n",
        "except Exception as e:\n",
        "    print('Parquet 保存失败，改用 CSV:', str(e))\n",
        "    df.to_csv(clean_csv, index=False)\n",
        "    print('清洗数据已保存至: ', clean_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 配置构念与量表条目（根据你的列名修改）\n",
        "# 请将下方字典的键替换成你数据中对应的列名列表\n",
        "CONSTRUCTS = {\n",
        "    # 示例：感知威胁严重性（Perceived Threat Severity）\n",
        "    'pts': [\n",
        "        # 'PTS1', 'PTS2', 'PTS3', ...\n",
        "    ],\n",
        "    # 示例：应对效能（Coping Efficacy）\n",
        "    'ce': [\n",
        "        # 'CE1', 'CE2', 'CE3', ...\n",
        "    ],\n",
        "    # 示例：采纳意向（Adoption Intention）\n",
        "    'ai': [\n",
        "        # 'AI1', 'AI2', 'AI3', ...\n",
        "    ],\n",
        "    # 示例：感知收益（Perceived Benefits）\n",
        "    'pb': [\n",
        "        # 'PB1', 'PB2', 'PB3', ...\n",
        "    ],\n",
        "}\n",
        "\n",
        "# 其它关键变量（根据列名修改）\n",
        "KEY_VARS = {\n",
        "    'zta_familiarity': 'ZTA Familiarity',\n",
        "    'zta_necessity': 'Perceived Necessity of ZTA',\n",
        "    'zta_satisfaction': 'Satisfaction with Current Security',\n",
        "    'zta_adoption': 'ZTA Adoption',  # 0/1 或 类别\n",
        "    'org_size': 'Organization Size',\n",
        "    'industry': 'Industry'\n",
        "}\n",
        "\n",
        "print('请检查并修改 CONSTRUCTS 与 KEY_VARS 中的列名以匹配数据集。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 信度分析：按构念计算 Cronbach's Alpha\n",
        "alpha_rows = []\n",
        "for name, cols in CONSTRUCTS.items():\n",
        "    cols = [c for c in cols if c in df.columns]\n",
        "    if len(cols) >= 2:\n",
        "        alpha = cronbach_alpha(df[cols])\n",
        "        alpha_rows.append({'construct': name, 'k_items': len(cols), 'alpha': alpha})\n",
        "\n",
        "alpha_df = pd.DataFrame(alpha_rows)\n",
        "print(alpha_df)\n",
        "alpha_df.to_csv(os.path.join(OUTPUT_DIR, 'cronbach_alpha.csv'), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 探索性因子分析（EFA）：以感知收益（PB）与采纳意向（AI）为例\n",
        "# 将需要做 EFA 的条目集合到一起（可按构念分别做）\n",
        "EFA_TARGETS = {\n",
        "    'pb': [c for c in CONSTRUCTS.get('pb', []) if c in df.columns],\n",
        "    'ai': [c for c in CONSTRUCTS.get('ai', []) if c in df.columns],\n",
        "}\n",
        "\n",
        "efa_results = {}\n",
        "for key, cols in EFA_TARGETS.items():\n",
        "    if len(cols) >= 3:\n",
        "        sub = df[cols].dropna()\n",
        "        # KMO 与 Bartlett 球形度检验\n",
        "        chi_square_value, p_value = calculate_bartlett_sphericity(sub)\n",
        "        kmo_all, kmo_model = calculate_kmo(sub)\n",
        "        # 根据特征值>1 估计因子数\n",
        "        fa_check = FactorAnalyzer(n_factors=min(6, len(cols)), rotation=None)\n",
        "        fa_check.fit(sub)\n",
        "        ev, v = fa_check.get_eigenvalues()\n",
        "        n_factors = int((ev > 1).sum())\n",
        "        n_factors = max(1, n_factors)\n",
        "        # 最终旋转（varimax）\n",
        "        fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax')\n",
        "        fa.fit(sub)\n",
        "        loadings = pd.DataFrame(fa.loadings_, index=cols, columns=[f'F{i+1}' for i in range(n_factors)])\n",
        "        efa_results[key] = {\n",
        "            'bartlett_chi2': float(chi_square_value), 'bartlett_p': float(p_value),\n",
        "            'kmo_model': float(kmo_model),\n",
        "            'eigenvalues': ev.tolist(),\n",
        "            'n_factors': int(n_factors),\n",
        "            'loadings': loadings\n",
        "        }\n",
        "        loadings.to_csv(os.path.join(OUTPUT_DIR, f'efa_loadings_{key}.csv'))\n",
        "        pd.DataFrame({'eigenvalue': ev}).to_csv(os.path.join(OUTPUT_DIR, f'efa_eigenvalues_{key}.csv'), index=False)\n",
        "\n",
        "# 保存一个简要的 EFA JSON 概览\n",
        "with open(os.path.join(OUTPUT_DIR, 'efa_summary.json'), 'w') as f:\n",
        "    json.dump({k: {kk: vv for kk, vv in d.items() if kk != 'loadings'} for k, d in efa_results.items()}, f, indent=2)\n",
        "\n",
        "print('EFA 完成。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 描述统计：人口统计与核心变量\n",
        "# 人口统计列（请根据你的列名调整）\n",
        "DEMOGRAPHICS = ['Gender', 'Age', 'Education', KEY_VARS['org_size'], KEY_VARS['industry']]\n",
        "\n",
        "# 数值型核心变量（须已重编码或本身为数值）\n",
        "NUMERIC_VARS = []\n",
        "for key in ['zta_familiarity', 'zta_necessity', 'zta_satisfaction']:\n",
        "    col = KEY_VARS.get(key)\n",
        "    if col in df.columns:\n",
        "        NUMERIC_VARS.append(col)\n",
        "\n",
        "# 概要统计\n",
        "desc_num = df[NUMERIC_VARS].describe().T if NUMERIC_VARS else pd.DataFrame()\n",
        "print(desc_num)\n",
        "desc_num.to_csv(os.path.join(OUTPUT_DIR, 'desc_numeric.csv'))\n",
        "\n",
        "# 分类频数\n",
        "cat_reports = {}\n",
        "for col in DEMOGRAPHICS:\n",
        "    if col in df.columns:\n",
        "        counts = df[col].value_counts(dropna=False)\n",
        "        cat_reports[col] = counts\n",
        "        counts.to_csv(os.path.join(OUTPUT_DIR, f'freq_{col}.csv'))\n",
        "\n",
        "print('描述统计完成。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 若无多题Likert构念：输出说明并跳过 Alpha/EFA\n",
        "has_any_construct = any(len(v) >= 2 for v in CONSTRUCTS.values())\n",
        "if not has_any_construct:\n",
        "    note = '未检测到可用于 Cronbach/ EFA 的多题Likert构念；跳过信度与EFA。可改为对二元多选做MCA或聚类。'\n",
        "    with open(os.path.join(OUTPUT_DIR, 'note_no_constructs.txt'), 'w') as f:\n",
        "        f.write(note)\n",
        "    print(note)\n",
        "else:\n",
        "    print('检测到多题构念，将在前述单元完成 Alpha/EFA。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q3.3 多选题拆分为哑变量，并做共现分析\n",
        "multi_col = 'Q3.3'\n",
        "if multi_col in df.columns:\n",
        "    s = df[multi_col].astype(str).fillna('')\n",
        "    # 以逗号分隔，清洗空白\n",
        "    split = s.str.get_dummies(sep=',')\n",
        "    # 规范列名\n",
        "    split.columns = [c.strip() for c in split.columns]\n",
        "    # 去掉空字符串列\n",
        "    if '' in split.columns:\n",
        "        split = split.drop(columns=[''])\n",
        "    # 保存二元矩阵\n",
        "    split.to_csv(os.path.join(OUTPUT_DIR, 'Q3_3_dummies.csv'), index=False)\n",
        "\n",
        "    # 简单共现矩阵\n",
        "    co = split.T @ split\n",
        "    np.fill_diagonal(co.values, 0)\n",
        "    co.to_csv(os.path.join(OUTPUT_DIR, 'Q3_3_cooccurrence.csv'))\n",
        "    print('Q3.3 多选拆分与共现完成。')\n",
        "else:\n",
        "    print('未找到 Q3.3 列，跳过多选处理。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 逻辑回归：adopt_zta ~ 熟悉度 + 必要性 + 满意度 + 规模 + 行业\n",
        "from statsmodels.discrete.discrete_model import Logit\n",
        "from patsy import dmatrices\n",
        "\n",
        "if 'adopt_zta' not in df.columns and 'Q2.8' in df.columns:\n",
        "    df['adopt_zta'] = (df['Q2.8'].astype(str).str.contains('Zero Trust Architecture', case=False, na=False)).astype(int)\n",
        "\n",
        "vars_available = [c for c in ['Q3.1_1','Q3.2_1','Q2.9_1','Q1.2','Q1.4','adopt_zta'] if c in df.columns]\n",
        "if set(['adopt_zta','Q3.2_1']).issubset(vars_available):\n",
        "    # 将类别变量设为分类特征\n",
        "    for cat in ['Q1.2','Q1.4']:\n",
        "        if cat in df.columns:\n",
        "            df[cat] = df[cat].astype('category')\n",
        "    # 使用 Q(\"列名\") 包裹包含点号的列名，避免 patsy 解析错误\n",
        "    formula = 'adopt_zta ~ Q(\"Q3.1_1\") + Q(\"Q3.2_1\") + Q(\"Q2.9_1\") + C(Q(\"Q1.2\")) + C(Q(\"Q1.4\"))'\n",
        "    y, X = dmatrices(formula, df, return_type='dataframe')\n",
        "    # 二项逻辑回归（去除缺失）\n",
        "    yv = y.iloc[:,0]\n",
        "    model = Logit(yv, X).fit(disp=False)\n",
        "    summ = model.summary2().as_text()\n",
        "    with open(os.path.join(OUTPUT_DIR, 'logit_adopt.txt'), 'w') as f:\n",
        "        f.write(summ)\n",
        "    print('逻辑回归完成，结果已保存 logit_adopt.txt')\n",
        "else:\n",
        "    print('缺少回归所需变量，跳过逻辑回归。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MCA：基于 Q3.3 多选哑变量的多重对应分析\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import prince\n",
        "\n",
        "multi_col = 'Q3.3'\n",
        "if multi_col in df.columns:\n",
        "    s = df[multi_col].astype(str).fillna('')\n",
        "    split = s.str.get_dummies(sep=',')\n",
        "    split.columns = [c.strip() for c in split.columns]\n",
        "    if '' in split.columns:\n",
        "        split = split.drop(columns=[''])\n",
        "    # 过滤全零列（无人选择的选项）\n",
        "    split = split.loc[:, split.sum(axis=0) > 0]\n",
        "    # 至少需要两个选项\n",
        "    if split.shape[1] >= 2 and split.shape[0] >= 5:\n",
        "        mca = prince.MCA(n_components=2, copy=True, check_input=True, random_state=42)\n",
        "        mca = mca.fit(split)\n",
        "        row_coords = mca.row_coordinates(split)\n",
        "        col_coords = mca.column_coordinates(split)\n",
        "\n",
        "        row_coords.to_csv(os.path.join(OUTPUT_DIR, 'mca_Q3_3_row_coords.csv'), index=False)\n",
        "        col_coords.to_csv(os.path.join(OUTPUT_DIR, 'mca_Q3_3_col_coords.csv'))\n",
        "\n",
        "        # 解释方差\n",
        "        try:\n",
        "            eig = pd.DataFrame({'eigenvalue': mca.eigenvalues_})\n",
        "        except Exception:\n",
        "            # 某些版本可能没有 eigenvalues_，使用 explained_inertia_\n",
        "            ev = np.array(mca.explained_inertia_)\n",
        "            eig = pd.DataFrame({'eigenvalue': ev})\n",
        "        eig.to_csv(os.path.join(OUTPUT_DIR, 'mca_Q3_3_eigenvalues.csv'), index=False)\n",
        "\n",
        "        # 简单散点图（行点）\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.scatter(row_coords.iloc[:,0], row_coords.iloc[:,1], s=20, alpha=0.6)\n",
        "        plt.axhline(0, color='gray', lw=0.5)\n",
        "        plt.axvline(0, color='gray', lw=0.5)\n",
        "        plt.title('MCA of Q3.3 (Rows)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'mca_Q3_3_rows.png'), dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "        # 选项点图\n",
        "        plt.figure(figsize=(5,4))\n",
        "        plt.scatter(col_coords.iloc[:,0], col_coords.iloc[:,1], s=30, c='C1')\n",
        "        for name, (x,y) in col_coords.iloc[:, :2].iterrows():\n",
        "            plt.text(x, y, str(name)[:20], fontsize=7)\n",
        "        plt.axhline(0, color='gray', lw=0.5)\n",
        "        plt.axvline(0, color='gray', lw=0.5)\n",
        "        plt.title('MCA of Q3.3 (Options)')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, 'mca_Q3_3_options.png'), dpi=150)\n",
        "        plt.close()\n",
        "        print('MCA 完成。')\n",
        "    else:\n",
        "        print('Q3.3 选项数或样本不足，跳过 MCA。')\n",
        "else:\n",
        "    print('未找到 Q3.3 列，跳过 MCA。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 相关分析：ZTA 熟悉度、必要性、满意度\n",
        "corr_cols = [KEY_VARS[k] for k in ['zta_familiarity', 'zta_necessity', 'zta_satisfaction'] if KEY_VARS[k] in df.columns]\n",
        "if len(corr_cols) >= 2:\n",
        "    sub = df[corr_cols].dropna()\n",
        "    corr = sub.corr(method='pearson')\n",
        "    print(corr)\n",
        "    corr.to_csv(os.path.join(OUTPUT_DIR, 'correlation_core.csv'))\n",
        "    # 显示散点图矩阵\n",
        "    sns.pairplot(sub)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'pairplot_core.png'), dpi=150)\n",
        "    plt.close()\n",
        "else:\n",
        "    print('相关分析所需变量不足。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ANOVA 与 卡方检验\n",
        "adoption_col = KEY_VARS.get('zta_adoption')\n",
        "size_col = KEY_VARS.get('org_size')\n",
        "industry_col = KEY_VARS.get('industry')\n",
        "\n",
        "# 若采纳为数值（0/1），可做组间均值差异（对核心连续变量）\n",
        "if adoption_col in df.columns:\n",
        "    # 示例：不同采纳状态下的 ZTA 必要性感知差异\n",
        "    target = KEY_VARS.get('zta_necessity')\n",
        "    if target in df.columns:\n",
        "        tmp = df[[adoption_col, target]].dropna()\n",
        "        if tmp[adoption_col].nunique() >= 2:\n",
        "            groups = [g[target].values for _, g in tmp.groupby(adoption_col)]\n",
        "            fval, pval = stats.f_oneway(*groups)\n",
        "            print('ANOVA (ZTA 采纳 -> 必要性): F=%.3f, p=%.4g' % (fval, pval))\n",
        "\n",
        "# 行业 × 采纳 卡方\n",
        "if industry_col in df.columns and adoption_col in df.columns:\n",
        "    ct = pd.crosstab(df[industry_col], df[adoption_col])\n",
        "    chi2, p, dof, exp = stats.chi2_contingency(ct)\n",
        "    print('Chi-square (行业 × 采纳): chi2=%.3f, p=%.4g, dof=%d' % (chi2, p, dof))\n",
        "    ct.to_csv(os.path.join(OUTPUT_DIR, 'chisq_industry_adoption_ct.csv'))\n",
        "\n",
        "# 规模 × 采纳 卡方\n",
        "if size_col in df.columns and adoption_col in df.columns:\n",
        "    ct = pd.crosstab(df[size_col], df[adoption_col])\n",
        "    chi2, p, dof, exp = stats.chi2_contingency(ct)\n",
        "    print('Chi-square (规模 × 采纳): chi2=%.3f, p=%.4g, dof=%d' % (chi2, p, dof))\n",
        "    ct.to_csv(os.path.join(OUTPUT_DIR, 'chisq_size_adoption_ct.csv'))\n",
        "\n",
        "print('ANOVA/卡方检验完成。')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 可选：导出关键图表（直方图/箱线图/相关热力图）\n",
        "if NUMERIC_VARS:\n",
        "    # 直方图\n",
        "    for col in NUMERIC_VARS:\n",
        "        plt.figure(figsize=(5,3))\n",
        "        sns.histplot(df[col].dropna(), kde=True)\n",
        "        plt.title(f'Distribution - {col}')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUTPUT_DIR, f'hist_{col}.png'), dpi=150)\n",
        "        plt.close()\n",
        "\n",
        "    # 相关热力图\n",
        "    corr = df[NUMERIC_VARS].corr()\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title('Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'corr_heatmap.png'), dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "print('图表导出完成。')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
